# Лабораторная работа 3
Выполнил Гораш Вячеслав Игоревич, гр. P41404

## Задание
1. Реализовать систему классификации согласно описанию, используя не
менее трех различных архитектур нейронной сети.
2. Сравнить качество работы, скорость и количество потребляемой памяти
для каждой архитектуры.
3. Сделать отчёт в виде readme на GitHub, там же должен быть выложен
исходный код.

## Данные
Для обучения и тестирования сетей использовался датасет MNIST. Он состоит из изображений рукописных цифр. Каждое изображение монохромное, 28 на 28 пикселей. 60000 тренировочных примеров, 10000 тестовых примеров

## Архитектуры сетей
1. Полносвязная нейронная сеть. Архитектура 784 -> 512 -> 256 -> 128 -> 64 -> 32 -> 10. Функции активации везде ReLu, на последнем слое Softmax
2. Написанная вручную сверточная нейронная сеть. Архитектура conv3x3_5_channels -> maxpool2x2 -> conv3x3_10_channels -> maxpool2x2 -> flatten -> 10 -> 10. Активация везде сигмоидальная, последний слой Softmax
3. Нейросеть на фреймворке tensorflow, повторяющая архитектуру сети 2.
4. Нейросеть на фреймворке tensorflow. Архитектура conv3x3_32_channels -> maxpool2x2 -> conv3x3_64_channels -> maxpool2x2 -> flatten -> 64 -> 32 -> 10. Активация везде сигмоидальная, последний слой Softmax
5. Нейросеть, повторяющая архитектуру 4 с заменой сигмоиды в сверточных слоях на ReLu
6. Сеть на основе ResNet50 с дропаутом 0.5 и softmax слоем на 10 классов

## Функции потерь
Во всех нейронных сетях, написанных на фреймворке tensorflow, используется категориальная кросс-энтропия. В сети, написанной без фреймворков, используется среднеквадратическая ошибка

## Сравнение архитектур
| Архитектура | Время 1 эпохи, c | Полное время обучения, с | Топ-1 точность | Топ-5 точность |
|-------------|------------------|--------------------------|----------------|----------------|
| 1           | 5                | 41                       | 0.9801         | 0.9924         |
| 2           | 1222             | -                        | 0.4254         | 0.6051         |
| 3           | 6                | 39                       | 0.9648         | 0.9875         |
| 4           | 4                | 39                       | 0.9856         | 0.9968         |
| 5           | 4                | 39                       | 0.9909         | 0.9982         |
| 6           | 44               | 413                      | 0.9771         | 0.9944         |

## Выводы
В ходе работы были рассмотрены несколько архитектур для классификации изображений небольшого размера.
Полносвязная архитектура может применяться для изображений небольшого размера, однако даже в случае MNIST на первом слое будет более 700 нейронов. И их количество возрастает с квадратической зависимостью от размера изображения.
Сверточные нейронные сети не имеют проблемы роста параметров. Важно следить за размером сети и количеством параметров чтобы избежать переобучения. Крупные архитектуры типа ResNet или VGG являются избыточными при малом количестве изображений и малом количестве классов